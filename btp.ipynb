{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"K-XuL0WCJRLV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692970212609,"user_tz":-330,"elapsed":30129,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}},"outputId":"465339e0-2c0a-4a56-a329-68121ad93c68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UeK1JurCJTso"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision import models"]},{"cell_type":"code","source":["# import pandas as pd\n","# import torch\n","# from torchvision import transforms\n","\n","# # read the Excel file into a pandas dataframe\n","# data = pd.read_excel('/content/final.xlsx')\n","\n","# # extract the image drive link and target values from the dataframe\n","# image_links = data.iloc[:, 0].tolist()\n","# targets = data.iloc[:, 1:3].values\n","\n","# # define a transformation to apply to the images\n","# transform = transforms.Compose([\n","#     transforms.Resize((512, 512)),  # resize the image to 224x224\n","#     transforms.ToTensor()  # convert the image to a PyTorch tensor\n","# ])"],"metadata":{"id":"OCxl5g67RIBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_links[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ZuqYttyDnxeu","executionInfo":{"status":"ok","timestamp":1692971409529,"user_tz":-330,"elapsed":437,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}},"outputId":"53efd998-6e02-4fd5-8c5e-8bdbabb07869"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/btp/day3-01.png'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torchvision import transforms\n","import numpy as np\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# read the Excel file into a pandas dataframe\n","data = pd.read_excel('/content/btp.xlsx')\n","\n","# extract the image drive link and target values from the dataframe\n","image_links = data.iloc[:, 0].tolist()\n","image_links = [i[:-3] + 'png' for i in image_links]\n","targets = data.iloc[:, 1].values\n","targets2 = data.iloc[:, 2].values\n","print(targets)\n","print(targets2)\n","minT2 =  np.min(targets2)\n","maxT2 = np.max(targets2)\n","\n","minT1 =  np.min(targets)\n","maxT1 = np.max(targets)\n","\n","'conte'\n","# Min-Max normalization\n","# targets2 = (targets2 - minT2) / (maxT2 -minT2)\n","# targets = (targets -minT1) / (maxT1 - minT1)\n","\n","# define a transformation to apply to the images\n","transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor()  # convert the image to a PyTorch tensor\n","])"],"metadata":{"id":"Nh5BmQDONEHw","executionInfo":{"status":"ok","timestamp":1692970261586,"user_tz":-330,"elapsed":2169,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"affa3045-1cce-4faa-d6e7-c04becfe35df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 2 2 3 3 2 3 2 2 3 2 3 2 3 2 3 4 5 4 3 4 2 5 4 5 4 0 5 5 5 5 5 5 5 5 5 5\n"," 4 2 4 4 4 4 0 4 4 4 4 5 0]\n","[2 2 2 3 3 2 3 3 3 3 2 3 2 4 4 4 4 4 4 4 5 4 5 5 5 5 0 4 4 4 3 4 5 5 4 5 5\n"," 4 3 4 4 4 3 2 3 4 4 3 5 3]\n"]}]},{"cell_type":"code","source":["from PIL import Image\n"],"metadata":{"id":"Fxfh8aiLSgcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a dataset object\n","class MyDataset(torch.utils.data.Dataset):\n","    def  __init__(self, image_links, targets, target2, transform):\n","        self.image_links = image_links\n","        self.targets = targets\n","        self.targets2 = targets2\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_links)\n","\n","    def __getitem__(self, idx):\n","        # load the image from the drive link\n","        image = Image.open(self.image_links[idx]).convert('RGB')\n","\n","        # apply the transformation to the image\n","        image = self.transform(image)\n","\n","        # get the target values for the image\n","        target = torch.tensor([self.targets[idx],self.targets2[idx]], dtype=torch.float32)\n","        # print(self.targets2[idx])\n","\n","        # target2 = torch.tensor(float(self.targets2[idx]), dtype=torch.float32)\n","\n","        # print(target.size())\n","        # print(target2.size())\n","        # print(target2)\n","\n","\n","        # concatenated_target = torch.cat((target, target2.view(1)), dim=0)\n","\n","        return image,   target\n","\n","\n","\n","\n","# create a dataset object from the data\n","dataset = MyDataset(image_links, targets,targets2, transform)\n","\n","from torch.utils.data import random_split\n","\n","# Assuming you have a complete dataset named 'dataset'\n","dataset_size = len(dataset)\n","train_size = int(0.8 * dataset_size)\n","val_size = dataset_size - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","\n","# create a dataloader object to load the data in batches\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n","valloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True)"],"metadata":{"id":"qTawFczcRL9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in trainloader:\n","  print(i)\n","  break"],"metadata":{"id":"KqEFhQfZSKRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692970262814,"user_tz":-330,"elapsed":1230,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}},"outputId":"247d7776-37e9-472e-e85e-a7cc2605f837"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[[[0.9804, 0.9569, 0.9608,  ..., 0.9961, 0.9961, 1.0000],\n","          [0.9216, 0.8745, 0.8902,  ..., 0.9804, 0.9765, 0.9882],\n","          [0.9412, 0.8980, 0.9098,  ..., 1.0000, 0.9922, 0.9922],\n","          ...,\n","          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 0.9922, 0.9922],\n","          [0.9882, 0.9765, 0.9765,  ..., 0.9804, 0.9765, 0.9882],\n","          [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000]],\n","\n","         [[0.9804, 0.9569, 0.9608,  ..., 0.9961, 0.9961, 1.0000],\n","          [0.9216, 0.8745, 0.8902,  ..., 0.9804, 0.9765, 0.9882],\n","          [0.9412, 0.8980, 0.9098,  ..., 1.0000, 0.9922, 0.9922],\n","          ...,\n","          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 0.9922, 0.9922],\n","          [0.9882, 0.9765, 0.9765,  ..., 0.9804, 0.9765, 0.9882],\n","          [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000]],\n","\n","         [[0.9804, 0.9569, 0.9608,  ..., 0.9961, 0.9961, 1.0000],\n","          [0.9216, 0.8745, 0.8902,  ..., 0.9804, 0.9765, 0.9882],\n","          [0.9412, 0.8980, 0.9098,  ..., 1.0000, 0.9922, 0.9922],\n","          ...,\n","          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 0.9922, 0.9922],\n","          [0.9882, 0.9765, 0.9765,  ..., 0.9804, 0.9765, 0.9882],\n","          [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000]]],\n","\n","\n","        [[[0.7255, 0.6667, 0.6902,  ..., 0.7569, 0.7725, 0.8314],\n","          [0.6392, 0.5961, 0.6078,  ..., 0.6627, 0.6627, 0.7255],\n","          [0.6549, 0.6078, 0.6078,  ..., 0.6784, 0.6745, 0.7294],\n","          ...,\n","          [0.8000, 0.7412, 0.7451,  ..., 0.7490, 0.7490, 0.8157],\n","          [0.7804, 0.7176, 0.7333,  ..., 0.7451, 0.7451, 0.8118],\n","          [0.8824, 0.8275, 0.8353,  ..., 0.8510, 0.8510, 0.9020]],\n","\n","         [[0.7255, 0.6667, 0.6902,  ..., 0.7569, 0.7725, 0.8314],\n","          [0.6392, 0.5961, 0.6078,  ..., 0.6627, 0.6627, 0.7255],\n","          [0.6549, 0.6078, 0.6078,  ..., 0.6784, 0.6745, 0.7294],\n","          ...,\n","          [0.8000, 0.7412, 0.7451,  ..., 0.7490, 0.7490, 0.8157],\n","          [0.7804, 0.7176, 0.7333,  ..., 0.7451, 0.7451, 0.8118],\n","          [0.8824, 0.8275, 0.8353,  ..., 0.8510, 0.8510, 0.9020]],\n","\n","         [[0.7255, 0.6667, 0.6902,  ..., 0.7569, 0.7725, 0.8314],\n","          [0.6392, 0.5961, 0.6078,  ..., 0.6627, 0.6627, 0.7255],\n","          [0.6549, 0.6078, 0.6078,  ..., 0.6784, 0.6745, 0.7294],\n","          ...,\n","          [0.8000, 0.7412, 0.7451,  ..., 0.7490, 0.7490, 0.8157],\n","          [0.7804, 0.7176, 0.7333,  ..., 0.7451, 0.7451, 0.8118],\n","          [0.8824, 0.8275, 0.8353,  ..., 0.8510, 0.8510, 0.9020]]],\n","\n","\n","        [[[0.8039, 0.7412, 0.7490,  ..., 0.8549, 0.8471, 0.9059],\n","          [0.7020, 0.6627, 0.6588,  ..., 0.7529, 0.7451, 0.8196],\n","          [0.7216, 0.6627, 0.6667,  ..., 0.7608, 0.7569, 0.8275],\n","          ...,\n","          [0.8353, 0.7961, 0.8039,  ..., 0.8588, 0.8667, 0.9216],\n","          [0.8275, 0.7882, 0.7882,  ..., 0.8549, 0.8627, 0.9137],\n","          [0.9255, 0.8902, 0.9098,  ..., 0.9412, 0.9490, 0.9647]],\n","\n","         [[0.8039, 0.7412, 0.7490,  ..., 0.8549, 0.8471, 0.9059],\n","          [0.7020, 0.6627, 0.6588,  ..., 0.7529, 0.7451, 0.8196],\n","          [0.7216, 0.6627, 0.6667,  ..., 0.7608, 0.7569, 0.8275],\n","          ...,\n","          [0.8353, 0.7961, 0.8039,  ..., 0.8588, 0.8667, 0.9216],\n","          [0.8275, 0.7882, 0.7882,  ..., 0.8549, 0.8627, 0.9137],\n","          [0.9255, 0.8902, 0.9098,  ..., 0.9412, 0.9490, 0.9647]],\n","\n","         [[0.8039, 0.7412, 0.7490,  ..., 0.8549, 0.8471, 0.9059],\n","          [0.7020, 0.6627, 0.6588,  ..., 0.7529, 0.7451, 0.8196],\n","          [0.7216, 0.6627, 0.6667,  ..., 0.7608, 0.7569, 0.8275],\n","          ...,\n","          [0.8353, 0.7961, 0.8039,  ..., 0.8588, 0.8667, 0.9216],\n","          [0.8275, 0.7882, 0.7882,  ..., 0.8549, 0.8627, 0.9137],\n","          [0.9255, 0.8902, 0.9098,  ..., 0.9412, 0.9490, 0.9647]]],\n","\n","\n","        [[[0.6471, 0.5961, 0.6039,  ..., 0.6588, 0.6510, 0.7176],\n","          [0.5569, 0.5255, 0.5333,  ..., 0.5765, 0.5725, 0.6157],\n","          [0.5843, 0.5373, 0.5373,  ..., 0.5882, 0.5804, 0.6235],\n","          ...,\n","          [0.6784, 0.6196, 0.6275,  ..., 0.6431, 0.6431, 0.7059],\n","          [0.6667, 0.6118, 0.6157,  ..., 0.6431, 0.6471, 0.6863],\n","          [0.7647, 0.6941, 0.6980,  ..., 0.7216, 0.7255, 0.8000]],\n","\n","         [[0.6471, 0.5961, 0.6039,  ..., 0.6588, 0.6510, 0.7176],\n","          [0.5569, 0.5255, 0.5333,  ..., 0.5765, 0.5725, 0.6157],\n","          [0.5843, 0.5373, 0.5373,  ..., 0.5882, 0.5804, 0.6235],\n","          ...,\n","          [0.6784, 0.6196, 0.6275,  ..., 0.6431, 0.6431, 0.7059],\n","          [0.6667, 0.6118, 0.6157,  ..., 0.6431, 0.6471, 0.6863],\n","          [0.7647, 0.6941, 0.6980,  ..., 0.7216, 0.7255, 0.8000]],\n","\n","         [[0.6471, 0.5961, 0.6039,  ..., 0.6588, 0.6510, 0.7176],\n","          [0.5569, 0.5255, 0.5333,  ..., 0.5765, 0.5725, 0.6157],\n","          [0.5843, 0.5373, 0.5373,  ..., 0.5882, 0.5804, 0.6235],\n","          ...,\n","          [0.6784, 0.6196, 0.6275,  ..., 0.6431, 0.6431, 0.7059],\n","          [0.6667, 0.6118, 0.6157,  ..., 0.6431, 0.6471, 0.6863],\n","          [0.7647, 0.6941, 0.6980,  ..., 0.7216, 0.7255, 0.8000]]]]), tensor([[0., 3.],\n","        [5., 5.],\n","        [5., 4.],\n","        [2., 4.]])]\n"]}]},{"cell_type":"code","source":["\n","# Define the device to use for training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"MtrFOoMgYkun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the CNN\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.cnn = models.resnet18(pretrained=True)\n","        self.cnn.fc = nn.Identity()\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","# Define the FCN\n","class FCN(nn.Module):\n","    def __init__(self):\n","        super(FCN, self).__init__()\n","        self.fc1 = nn.Linear(512, 256)\n","        self.fc2 = nn.Linear(256, 2)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Define the combined model\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.cnn = CNN()\n","        self.fcn = FCN()\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = self.fcn(x)\n","        return x\n","\n","# Define the loss function\n","criterion = nn.MSELoss()\n","\n","# Define the optimizer\n","model = Model()\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxPdt0H6KoXK","outputId":"72a95e3d-2c8d-491b-8977-0c5786f15334","executionInfo":{"status":"ok","timestamp":1692970269285,"user_tz":-330,"elapsed":6479,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 74.4MB/s]\n"]}]},{"cell_type":"code","source":["num_epochs=50"],"metadata":{"id":"CjjWxWLUYyt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Train the model\n","for epoch in range(num_epochs):\n","    for images, labels in trainloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        # print(outputs.shape)\n","        # print(labels.shape)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print the loss and accuracy for this batch\n","        with torch.no_grad():\n","            predictions = torch.round(outputs)\n","            accuracy = torch.mean((predictions == labels).float())\n","            print(\"Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%\".format(epoch+1, num_epochs, loss.item(), accuracy.item() * 100))\n"],"metadata":{"id":"iAuD7fGQKfZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = Image.open('/content/drive/MyDrive/btp/day14-03.png').convert('RGB')\n","\n","# apply the transformation to the image\n","image = transform(image)\n","image=image.unsqueeze(1).view(1,3,512,512)\n","\n","\n","outputs = model(image.to(device))\n"],"metadata":{"id":"q5EFUBnwbCKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHcOO3Z3bo53","outputId":"311ca9d5-432f-4757-8508-352bc1bc5bc7","executionInfo":{"status":"ok","timestamp":1692970409138,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3.9199, 3.8013]], device='cuda:0', grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Custom loss function for area prediction\n","class AreaLoss(nn.Module):\n","    def __init__(self):\n","        super(AreaLoss, self).__init__()\n","\n","    def forward(self, predicted_area, ground_truth_area):\n","        loss = torch.mean((predicted_area - ground_truth_area) ** 2)\n","        return loss\n"],"metadata":{"id":"pReu4FfIMO3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the CNN\n","class CNNarea(nn.Module):\n","    def __init__(self):\n","        super(CNNarea, self).__init__()\n","        self.cnn = models.resnet18(pretrained=True)\n","        self.cnn.fc = nn.Identity()\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","# Define the FCN\n","class FCNarea(nn.Module):\n","    def __init__(self):\n","        super(FCNarea, self).__init__()\n","        self.fc1 = nn.Linear(512, 256)\n","        self.fc2 = nn.Linear(256, 1)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Define the combined model\n","class Modelarea(nn.Module):\n","    def __init__(self):\n","        super(Modelarea, self).__init__()\n","        self.cnn = CNNarea()\n","        self.fcn = FCNarea()\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = self.fcn(x)\n","        return x\n","\n","# Define the loss function\n","criterion = AreaLoss()\n","\n","# Define the optimizer\n","modelarea = Modelarea()\n","modelarea = modelarea.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"11155bf7-87ef-4bc8-ad00-a613bf64c34b","executionInfo":{"status":"ok","timestamp":1683933246274,"user_tz":-330,"elapsed":1765,"user":{"displayName":"Ritish Khichi (B20BB031)","userId":"06759607463134617592"}},"id":"hNit7QCAMPQb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["num_epochs=50"],"metadata":{"id":"uuExRgcvMPQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Train the model\n","for epoch in range(num_epochs):\n","    for images, _,labels ,in trainloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = modelarea(images)\n","        print(outputs.shape)\n","        print(labels.shape)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print the loss and accuracy for this batch\n","        with torch.no_grad():\n","            predictions = torch.round(outputs)\n","            accuracy = torch.mean((predictions == labels).float())\n","            print(\"Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%\".format(epoch+1, num_epochs, loss.item(), accuracy.item() * 100))\n"],"metadata":{"id":"-KCRGIQ4MPQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UYp-xMOWOoJ2"},"execution_count":null,"outputs":[]}]}